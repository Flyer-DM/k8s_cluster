# Разворачивание трёхнодового k8s кластера

## Описание
Kubernetes кластер - это несколько машин (компьютеров), которые работают вместе для запуска и управления приложениями в контейнерах, которые мы называем
[микросервисами](https://dzen.ru/a/X-3C568ULwsX2F_N?utm_referer=yandex.ru). Kubernetes обеспечивает 
автоматическое масштабирование, балансировку нагрузки, управление ресурсами и высокую доступность приложений в кластере.
![kubernetes](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/018a3156-c9bc-4d5b-a42b-cc4e55213d60)
Кластер будет состоять из трёх виртуальных компьютеров (нод), установленных через [VirtualBox](https://www.virtualbox.org/): мастер-нода (управляющая кластером)
и две worker-ноды, на которых непосредственно можно разворачивать приложения. Машины будут работать на ОС astra linux версии [orel](https://dl.astralinux.ru/astra/stable/orel/iso/orel-current.iso).
Устанавливать все необходимые инструменты для управления кластером будем через [Ansible](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html) -
опенсорсную систему для управления конфигурациями на удалённых узлах в сети. Она позволяет автоматически настраивать и развёртывать програмное обеспечение по защищённому ssh-соединению.
С такой системой пропадает необходимость вручную устанавливать необходимые компоненты для работоспособности кластера на множестве одинаковых машин, то есть не нужно будет выполнять
одни и те же действия на одинаковых компьютерах. А все необходимые конфигурации для настройки самого кластера также не придётся прописывать на yaml (языке-разметки
сценариев, который понимает ansible), все сценарии уже подготовлены в репозитории [Kubespray](https://github.com/kubernetes-sigs/kubespray) 18 версии, что соответствует
k8s-кластеру версии v1.22.5. Kubespray версии 2.22 на данный момент является новейшим, но в ходе эксперементов сочетания разного ПО выяснилось, что версия 2.18 является самой устойчивой.

P.S. Я постараюсь описать всю установку пункт за пунктом, включая возможные ошибки, которые возникали у меня, так как работа с astra linux нетривиальна, и в большинстве гайдов,
которыми я пользовался, пропущены значимые моменты, однако я также могу случайно упустить некоторые пункты.

### 0)Начальная настройка машин
Минимальные требования для хост-машины: 16 Гб ОЗУ, 100+ Гб свободного места на жёстком диске, процессор от intel core i5, доступ в сеть.

Создадим три машины:

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/dcfb8fbe-9a6c-4c38-b07f-d3850ac57655)

Как вариант создать одну машину, провести все настройки до инициализации кластера, затем склонировать её два раза для создания worker-узлов (у клонов будет другой ip).

На мастер-ноде (m) установим 4 Гб ОЗУ, 4 ядра.
На рабочих нодах (w1, w2) установим по 2 Гб ОЗУ, 2 ядра (в случае клонирования просто понизить значения в настройках).
20 Гб виртуального жёсткого диска для всех трёх будет достаточно.
В настройках сети у всех трёх машин добавим второй адаптер: тип подключения - виртуальный адаптер хоста (для доступа в интернет и связывания машин в одну сеть).
На всех трёх нодах видеопамять выкрутим до 128 Мб (для настройки виртуального монитора, который по умолчанию будет в маленьком окне).
Можно также включить общий буфер обмена для всех машин.

Мастер нода:

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/8fa103a3-d60b-4618-8602-6b2feb5df06b)

Рабочие ноды:

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/8dd88d17-b0ea-4745-9b4c-85f675b494c6)

Включим в настройках сети "Виртуальные сети хоста" с автоматическим адаптером:

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/bb9f8ef9-4582-4753-820c-d03515d2e947)
![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/3583467b-76ff-40a4-88a0-722341fb5a94)

Запускаем машины. Первоначальную установку лучше всего проводить не в графическом режиме. Вписываем имя машины, имя пользователя, пароль (не менее 8 символов). Выбираем часовой пояс.
Метод разметки - "весь диск", выбираем наш диск, схема разметки - "все файлы в одном разделе", "закончить разметку и записать изменения на диск". Ядро для установки - дефолтное (уже выбрано будет).
ПО выбираем следующее:

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/a12c52d6-ab73-4e83-98ef-d6c804257e2f)

Дополнительные настройки:

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/37979c50-062d-44a3-a18c-08534b9f3571)

Устанавливаем системный загрузчик GRUB, выбираем "/dev/sda", выбираем "продолжить" после сообщения об удачной загрузке. Входим в созданный ранее аккаунт.

После предустановки ОС виртуальный монитор будет в маленьком окне. Это нужно исправить. Строка меню VB -> "Устройтва" -> "Подключить образ диска Дополнительной гостевой ОС".

Открываем терминал Fly через системные утилиты:
```
sudo passwd # устанавливаем пароль администратору (должен быть сложный - цифры, буквы, длиной не менее 8)
cd /media/cdrom0
sudo su # входим под правами админа
apt update && apt-get install bzip2 tar
apt install gcc make perl linux-headers-`uname -r`
sh ./VBoxLinuxAdditions.run
shutdown -r now # перезагружаем машину
```
После перезагрузки виртульный монитор будет нормально растягиваться на весь экран, заработает общий буфер обмена.
Если вместо графической оболочки открывается консоль, то после ввода логина и пароля прописываем следующую команду:
```
startx
```
Жмём правой кнопкой по рабочему столу -> "Свойства" -> "Блокировка" -> отключим блокировку экрана:

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/67065440-e6e5-4da3-a68c-3ebc475e6db5)

-> "Настройки электропитания" -> отключим "Выключение монитора", "События от кнопок", "Сон" в "Питание от сети", "Питание от батареи", "Низкий уровень заряда":

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/1a95cc59-a37b-4e97-ba7f-850938a18e79)

Данные действия с настройкой электропитания проделываем, так как наши машины являются виртуальными серверами, соотвественно уходить в сон или блокировать экран они
не должны.

Открываем консоль, заходим под администратором и подключаем репозитории со стандартными пакетами (которые разрабы astra linux решили зачем-то удалить):
```
sudo su
echo "deb https://deb.debian.org/debian/               buster         main contrib non-free" >> /etc/apt/sources.list
echo "deb https://security.debian.org/debian-security/ buster/updates main contrib non-free" >> /etc/apt/sources.list
echo "deb https://archive.debian.org/debian/ stretch main contrib non-free" >> /etc/apt/sources.list
apt update # пробуем обновить
```
Обновление неудачное:

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/a75ce475-495f-4461-b2ff-658a4b6da324)

Добавляем запрашиваемые ключи:
```
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 54404762BBB6E853
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 0E98404D386FA1D9
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517
apt update
```
Теперь списки пакетов для установки успешно обновлёны. На всех машинах узнаём ip-адрес в сети следующей командой:
```
hostname -I
```
![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/80631d45-a1bb-4fb8-b508-7e5a0056b1dc)

Их будет два, запоминаем второй. Далее редактируем файл с именами хостов следующим образом:
```
echo "192.168.56.111 m" >> /etc/hosts
echo "192.168.56.112 w1" >> /etc/hosts
echo "192.168.56.113 w2" >> /etc/hosts
```
где m - мастер нода, w1, w2 - наши рабочие ноды. 
Теперь нужно убедиться, что машины видят друг друга в сети, если уже сделали клонов. Пингуем с каждой машины каждую, выполнив следующие команды (соответственно все три машины должны быть
включены):
```
ping 192.168.56.111 -c 1
ping 192.168.56.112 -c 1
ping 192.168.56.113 -c 1
ping m -c 1
ping w1 -c 1
ping w2 -c 1
```
![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/34217396-c9e0-4eb6-84a5-bfb32cb42b35)

Как видно, каждая машина получила отправленный пакет.
Ansible и kubespray не могут работать на astra linux, потому что для нашей ОС не выпущены версии. Но программы можно обмануть, так как astra linux написана на основе ubuntu,
то следует заставить работающие на ней программы думать, что они работают на версии ubuntu. Для этого нужно поменять два системных файла: ```/etc/os-release``` и ```/etc/lsb-release```
со следующим содержимым:
```
NAME="Ubuntu"
VERSION="20.04.3 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.3 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
```
и
```
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=20.04
DISTRIB_CODENAME=focal
DISTRIB_DESCRIPTION="Ubuntu 20.04.3 LTS"
```
соответственно. После чего перезагружаем машины ```shutdown -r now```. Файлы **НЕЛЬЗЯ** удалять и создавать новые с данным содержимым, их следует отредактировать через nano/vim/mcedit и т.д.
После перезагрузки отключаем файл подкачки (одно из требований работы k8s кластера):
```
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab # отключаем автоматическое включение после перезагрузки
free -m # проверяем, что файл выключен (последняя строка должна быть с нулевыми значениями)
```
> [!NOTE]
> Делаем снимок состояния машин (Host+T) для возможности отката.

### 1)Установка Python
Ansible написан и работает на питоне, поэтому необходимо установить Python версии 3.7.3, на которой будут работать Ansible 2.18 и его модули, а также менеджер пакетов pip
для соответственной версии Python. pip по умолчанию ставится для Python 3.5, поэтому придётся устанавливать и переустанавливать его до необходимой версии.
```
apt-get install -y curl python3.7 python3.7-dev python3.7-distutils
update-alternatives --install /usr/bin/python python /usr/bin/python3.7 1
update-alternatives --set python /usr/bin/python3.7
curl -s https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python get-pip.py --force-reinstall && rm get-pip.py
python -V # проверяем версию питона
pip -V # проверяем версию менеджера пакетов
```
![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/0265a0dc-0465-474c-96f7-89ceee6509bf)

> [!NOTE]
> Делаем снимок состояния.

### 2)Установка необходимых пакетов
Данные пакеты ставим на все ноды:
```
apt install conntrack aufs-tools software-properties-common python3-apt apt-transport-https ethtool git -y
cd /usr/lib/python3/dist-packages
# добавим ссылки для избежания возможных багов с пакетом python3-apt:
ln -s apt_inst.cpython-35m-x86_64-linux-gnu.so apt_inst.so
ln -s apt_pkg.cpython-35m-x86_64-linux-gnu.so apt_pkg.so
```
Далее нужно настроить статиченые DNS-сервера для всех машин, они требуются для будущих обновлений пакетов и установки утилит управления кластером,
скачаем и включим необходимые сервисы:
```
apt install resolvconf
systemctl enable resolvconf.service
systemctl start resolvconf.service
systemctl start systemd-resolved.service
systemctl enable systemd-resolved.service
systemctl status resolvconf.service
systemctl status systemd-resolved.service
```
Статус должен быть зелённым со значением active. Добавим в файл ```nano /etc/resolvconf/resolv.conf.d/head``` строку ```nameserver 8.8.8.8```. Обновим файл:
```
resolvconf --enable-updates
resolvconf -u # обновим добавленное значение
cat /etc/resolv.conf # проверка изменения содержимого
```
![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/56b21328-5b32-4551-b736-284afc5af519)

> [!NOTE]
> Делаем снимок состояния.

### 3)Обмен ssh ключами с рабочими нодами
Ansible будет посылать разные команды на удалённые машины, в том числе на машину, на которой он работает. Для этого необходимо удалённое подключение. Наиболее удобный
вариант это подключение через ssh-ключи без необходимости ввода пароля. Настроим такое подключение. Тут уже параллельно должны быть включены все три узла. 
Ansible не может работать от имени администратора (root), поэтому дальнейшние действия выполним через обычного пользователя **ТОЛЬКО** на управляющем узле:
```
ssh-agent bash
ssh-keygen -t rsa -f ~/.ssh/id_rsa -P "" # генерация ключа
# далее "master" - имя пользователя компьютера
ssh-copy-id -i $HOME/.ssh/id_rsa.pub master@192.168.56.112 # перессылка ключа первому воркеру
ssh-copy-id -i $HOME/.ssh/id_rsa.pub master@192.168.56.113 # перессылка ключа второму воркеру
ssh-copy-id -i $HOME/.ssh/id_rsa.pub master@192.168.56.111 # перессылка ключа мастеру (самому себе) для подключения ansible
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys  # сохранение сгенерированного ключа для управляющей ноды
chmod og-wx ~/.ssh/authorized_keys # изменение прав доступа к ключу для ansible
```
> [!NOTE]
> Делаем снимок состояния.

### 4)Установка kubespray-2.18
Установим kubespray 18 версии на управляющую ноду, пользуясь официальным руководством: https://github.com/kubernetes-sigs/kubespray.
```
cd /home/master
mkdir k8s
cd k8s
git clone --branch v2.18.0 https://github.com/kubernetes-sigs/kubespray
cd kubespray/
pip install --ignore-installed -U -r requirements.txt # установка необходимых модулей для ansible
cp -rfp inventory/sample inventory/mycluster
# прописываем IP адреса наших машин
declare -a IPS=(192.168.56.111 192.168.56.112 192.168.56.113)
CONFIG_FILE=inventory/mycluster/hosts.yaml python contrib/inventory_builder/inventory.py ${IPS[@]}
```
Далее заменим содержимое файла ```/home/master/k8s/kubespray/inventory/mycluster/inventory.ini```:
```
[all]
m ansible_host=192.168.56.111 ip=192.168.56.111
w1 ansible_host=192.168.56.112 ip=192.168.56.112
w2 ansible_host=192.168.56.113 ip=192.168.56.113


[kube-master]
m


[etcd]
m


[kube-node]
m
w1
w2


[k8s-cluster:children]
kube-master
kube-node
```
Здесь мы определили ip-адреса нод и их dns имена (all), управляющую ноду (kube-master), место положения распределённой БД файлов конфигурации для управления
кластером (etcd), рабочие ноды (kube-node), в том числе и наша мастер-нода, то есть мы разрешаем разворачивать сервисы и на управляющей ноде. 
Поменяем права доступа настроенной директории:
```
chmod 777 -R /home/master/k8s/kubespray/ # раздача полных прав доступа
```
Перед инициализацией кластера поменяем один параметр в конфигурацинном файле кластера (без изменения при инициализации может появится ошибка):
```nano /home/master/k8s/kubespray/inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml```
меняем ```nodelocaldns_ip: 10.233.0.10```.

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/05304dce-ba33-435c-971a-377957e1d349)

> [!NOTE]
> Делаем снимок состояния.

### 5)Инициализация кластера
Кластер сконфигурирован, теперь можно запускать установку (все узлы нашего кластера должны быть включены).
Для запуска программы развёртывания нам понадобится перейти в директорию /home/master/k8s/ и выполнить следующую команду от лица **ПОЛЬЗОВАТЕЛЯ**:
```
ansible-playbook ./kubespray/cluster.yml -i ./kubespray/inventory/mycluster/inventory.ini --become --become-user=root
```
Установка занимает 20-30 минут, скорость интернет-соединения влияет, так как будут закачиваться файлы достаточного объёма,
все логи установки будут отображаться в терминале. Красные сообщения без надписи "FATAL" гласят о нефатальных ошибках, установщик пропустит их.
В конце мы должны увидеть что-то наподобие:

![Снимок экрана 2023-08-01 120654](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/c3de3ee5-9ae0-491b-8c04-4a61084a82f2)

Если везде значение параметра "failed" равно 0, значит кластер успешно инициализирован.

### 6)Настройка управления кластера
После инициализации **НЕ ВЫКЛЮЧАТЬ** наши машины. Нужно провести некоторые настройки, чтобы после перезагрузки машин все ноды снова объединялись в кластер:
```
sudo su # заходим под администратора
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" > /etc/environment
export KUBECONFIG=/etc/kubernetes/admin.conf
mkdir -p $HOME/.kube # создание пользовательской папки
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config # копирование конфига кубера в пользовательскую папку
sudo chown $(id -u):$(id -g) $HOME/.kube/config  # изменение прав доступа
```
Теперь мы можем управлять нашим кластером от администратора.

Для начала проверим, что наши ноды соединены и готовы для работы:

![Снимок экрана 2023-08-01 120746](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/fcca27a2-1a53-4e22-9985-eaeb8cc56f6f)

Статус "READY" означает, что всё готово, он может появится не сразу, следует подождать.

Теперь посмотрим, какие "кластерные" поды уже работают. Pod (под) в k8s - это самая маленькая и базовая единица развертывания приложения.
Он представляет собой группу одного или нескольких контейнеров, которые работают вместе на одном узле и имеют общую среду выполнения.
Каждый под имеет свой уникальный IP-адрес внутри кластера и может быть настроен для доступа к различным сервисам и ресурсам. Под управляется контроллером
репликации или деплойментом и может быть автоматически масштабирован в зависимости от нагрузки на приложение.
Выполним команду ```kubectl get pods -A```:

![Снимок экрана 2023-08-01 121011](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/72efc58b-858a-4dbb-8a9d-d6df22b19222)

Видим, что три системных пода в состоянии "не готов" со статусом ошибки. Исправим эту ошибку. Для этого на всех нодах нашего кластера выполним следующие действия:
изменим определённую строку в файле ```nano /etc/kubernetes/kubelet-config.yaml``` на ```resolvConf: "/etc/resolv.conf"```

![image](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/f29e163c-8a86-420a-8828-88ab012c507d)

> [!NOTE]
> Сделаем снимки состояния на всех машинах и перезагрузим их.

После перезагрузки машины должны соединиться в кластер. Заходим под администратором и проверяем это:

![Снимок экрана 2023-08-03 115059](https://github.com/Flyer-DM/k8s_cluster/assets/113033685/0432fd02-38fa-4795-8659-2a58568a1241)

Все поды заработали.  *Кластер полностью настроен и готов к развёртыванию микросервисов!* 
